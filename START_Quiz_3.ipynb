{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_needed = [\n",
    "    {\"thinkplot.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkplot.py\"},\n",
    "    {\"thinkstats2.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkstats2.py\"},\n",
    "]\n",
    "current_folder = os.getcwd()\n",
    "for f in files_needed:\n",
    "    for file_name, url in f.items():\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Downloading {file_name}\")\n",
    "            os.system(f\"curl {url} -o {current_folder}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from scipy import stats as ss\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Quiz 3</h1>\n",
    "\n",
    "Please fill in the bodies of the functions as specified. Please read the instructions closely and ask for clarification if needed. A few notes/tips:\n",
    "<ul>\n",
    "<li>Like all the functions we use, the function is a self contained thing. It takes in values as paramaters when called, and produces a return value. All of the inputs that may change should be in that function call, imagine your function being cut/pasted into some other file - it should not depend on anything outside of libraries that it may need. \n",
    "<li>Test your function with more than one function call, with different inputs. See an example in comments below the first question. \n",
    "<li>If something doesn't work, print or look at the varaibles window. The #1 skill that'll allow you to write usable code is the ability to find and fix errors. Printing a value out line by line so you can see how it changes, and looking for the step where something goes wrong is A-OK and pretty normal. It is boring. \n",
    "<li>Unless otherwise specified, you can use outside library functions to calculate things. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test Data</h1>\n",
    "\n",
    "You may notice there's no data specified or attached. You'll need to generate some test data if you want to test your functions. \n",
    "\n",
    "The easiest way to generate test data is to use some of the random functions to generate data that looks like what you need. Numpy random and scipy disributions .rvs functions are good places to look, we've also generated random data many times in the past. \n",
    "\n",
    "There is no specific requirement on what your data needs to be, it just needs to be good enough to test your function. If you pay attention to what exactly you're calculating and the criteria given, you should be able to create some suitable data for different tests. As an example, for the Hyp Test question, you need two sets of normal data. You can generate some in many ways, one is through scipy:\n",
    "<ul>\n",
    "<li>ss.norm.rvs(loc=0, scale=1, size=1, random_state=None)\n",
    "</ul>\n",
    "<p>\n",
    "Since you're checking if there's a significant difference between the two groups, you'd likely want multiple sets of data - two that are very close, so they will not show a difference, and two that are not close, so they will show a difference. Think about what you are checking, then just make some data that will allow you to test that. \n",
    "\n",
    "This should not be extremely difficult to code nor should it be super time consuming, the commands are pretty simple and generating random varaibles is pretty similar for any distribution. There is some though involved in saying \"what data do I need to check this?\" That's something that is pretty important in general, if we are creating something we need to make sure that it works in general, not just one example. Critically, there are not specific sets of data you need - almost anything will work. It is only there to let your functions run and see if they are correct. You don't need to aim for \"the perfect test data\" or anything like that, just make some data in a list, if it needs to be of a certain distribution, use that dist to get it; if the distribution doesn't matter, just make something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Generate Test Data ---\n",
    "np.random.seed(42) # Set seed for reproducibility\n",
    "\n",
    "# 1. Data for Ski Question (Observed counts)\n",
    "\n",
    "ski_observed = [150, 90, 30, 45, 85] \n",
    "\n",
    "# 2. Data for Hypothesis Testing (Normal distributions)\n",
    "\n",
    "group_A = ss.norm.rvs(loc=50, scale=5, size=100)\n",
    "\n",
    "group_B = ss.norm.rvs(loc=52, scale=5, size=100)\n",
    "\n",
    "group_C = ss.norm.rvs(loc=50, scale=5, size=100)\n",
    "\n",
    "# 3. Data for Flex Test (Non-normal data)\n",
    "# Exponential distribution\n",
    "non_normal_data = ss.expon.rvs(size=50)\n",
    "\n",
    "# 4. Data for Grades\n",
    "# Random integers between 40 and 100\n",
    "student_grades = np.random.randint(40, 100, size=200)\n",
    "\n",
    "print(\"Test data generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ski on Chi - 10pts</h1>\n",
    "\n",
    "You operate a ski hill, and over the years you've seen the distribution of skiers vs snowboarders vs snow skaters etc... change a bit. This is your first full open season since the pandemic hit. When you closed in early 2020, the distribution of your customer base was:\n",
    "<ul>\n",
    "<li>Skiers - 40%\n",
    "<li>Snowboarders - 20%\n",
    "<li>Snow Skaters - 5%\n",
    "<li>Non-Active (i.e. sit in the lodger) - 15%\n",
    "<li>Lesson takers - 20%\n",
    "</ul>\n",
    "\n",
    "You are seeing a different pattern now, but you are not sure if that is due to a change in what your customers want or due to just random chance. You want to be able to analytically tell if what you observe each week is a real change from that baseline above, or nothing to worry about. \n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values for the observed number of customers in each group, in the order indicated above. E.g. [35,25,10,10,20].\n",
    "<li>An alpha value (the cutoff criteria for a p-values)\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll return 3 results:\n",
    "<ul>\n",
    "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha. \n",
    "<li>The name of the category that MOST EXCEEDS the expectation. \n",
    "<li>The name of the cetegory that is MOST EXCEEDED BY the expectation. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skiCustomersChange(observedCustys, alpha=.05):\n",
    "    # Define the expected distribution percentages based on the prompt\n",
    "    \n",
    "    expected_ratios = [0.40, 0.20, 0.05, 0.15, 0.20]\n",
    "    category_names = [\"Skiers\", \"Snowboarders\", \"Snow Skaters\", \"Non-Active\", \"Lesson takers\"]\n",
    "    \n",
    "    # Calculate expected counts based on the total number of observed customers\n",
    "    total_observed = sum(observedCustys)\n",
    "    expected_counts = [ratio * total_observed for ratio in expected_ratios]\n",
    "    \n",
    "    # Perform Chi-Squared Goodness of Fit Test\n",
    "    chi_stat, p_value = ss.chisquare(f_obs=observedCustys, f_exp=expected_counts)\n",
    "    \n",
    "    # 1. Determine if significant difference exists\n",
    "    isSignificantDiff = p_value < alpha\n",
    "    \n",
    "   \n",
    "    differences = np.array(observedCustys) - np.array(expected_counts)\n",
    "    \n",
    "    # 2. Find category that MOST EXCEEDS expectation (Largest positive difference)\n",
    "    highest_diff_index = np.argmax(differences)\n",
    "    higherThanExp = category_names[highest_diff_index]\n",
    "    \n",
    "    # 3. Find category that is MOST EXCEEDED BY expectation (Largest negative difference)\n",
    "    # We want the minimum value (most negative)\n",
    "    lowest_diff_index = np.argmin(differences)\n",
    "    lowerThanExp = category_names[lowest_diff_index]\n",
    "\n",
    "    return isSignificantDiff, higherThanExp, lowerThanExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 Results:\n",
      "Significant Difference: True\n",
      "Category higher than expected: Snowboarders\n",
      "Category lower than expected: Non-Active\n",
      "------------------------------\n",
      "Test 2 (Prompt Example):\n",
      "Significant Difference: False\n",
      "Category higher than expected: Snowboarders\n",
      "Category lower than expected: Skiers\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Using the data generated above\n",
    "is_sig, high, low = skiCustomersChange(ski_observed, alpha=0.05)\n",
    "print(f\"Test 1 Results:\")\n",
    "print(f\"Significant Difference: {is_sig}\")\n",
    "print(f\"Category higher than expected: {high}\")\n",
    "print(f\"Category lower than expected: {low}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Test 2: Using the specific example from the prompt instructions\n",
    "# [35, 25, 10, 10, 20] vs expected percentages\n",
    "is_sig_2, high_2, low_2 = skiCustomersChange([35, 25, 10, 10, 20], alpha=0.05)\n",
    "print(f\"Test 2 (Prompt Example):\")\n",
    "print(f\"Significant Difference: {is_sig_2}\")\n",
    "print(f\"Category higher than expected: {high_2}\")\n",
    "print(f\"Category lower than expected: {low_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hypothesis Testing - 10pts</h2>\n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values - dataA and dataB. The data will be normally distributed. \n",
    "<li>An alpha value (the cutoff criteria for a p-values)\n",
    "<li>A power value (the likelihood of not getting a false negative)\n",
    "<li>An effect size value.\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll produce a tuple of 3 results:\n",
    "<ul>\n",
    "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha in a t-test.\n",
    "<li>A true/false assessment for if a hypothesis test has enough power to be reliable, measured by if the power you calculate is greater than the supplied power. \n",
    "<li>A true false assessment for if the data appears to show a significant difference in means, measured by if the Cohen effect size is greater than the supplied effect size. \n",
    "</ul>\n",
    "\n",
    "<b>Please report your responses in the format indicated in the template. As well, please report all true/false values as 1/0. 1 is True, 0 is false. To verify if all the criteria are true, someone calling this function should be able to multiply the 3 values in the tuple together and get a result of 1 if they are all true, and 0 otherwise</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "def strengthOfEffect(dataA, dataB, alpha=.05, power=.8, effectSize=.5):\n",
    "    \n",
    "    # 1. T-Test for significance\n",
    "    t_stat, p_val = ss.ttest_ind(dataA, dataB)\n",
    "    passedPtest = 1 if p_val < alpha else 0\n",
    "    \n",
    "    # 2. Calculate Cohen's d (Effect Size)\n",
    "    mean1, mean2 = np.mean(dataA), np.mean(dataB)\n",
    "    n1, n2 = len(dataA), len(dataB)\n",
    "    var1, var2 = np.var(dataA, ddof=1), np.var(dataB, ddof=1)\n",
    "    \n",
    "    # Calculate Pooled Standard Deviation\n",
    "    pooled_sd = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    cohens_d = abs(mean1 - mean2) / pooled_sd\n",
    "    \n",
    "    passedEffectSize = 1 if cohens_d > effectSize else 0\n",
    "    \n",
    "    # 3. Power Analysis\n",
    "    # Calculate the power of the test based on the calculated effect size\n",
    "    power_analysis = TTestIndPower()\n",
    "    ratio = n2 / n1 # Ratio of sample sizes\n",
    "    calculated_power = power_analysis.solve_power(effect_size=cohens_d, \n",
    "                                                  nobs1=n1, \n",
    "                                                  alpha=alpha, \n",
    "                                                  power=None, \n",
    "                                                  ratio=ratio)\n",
    "    \n",
    "    passedPower = 1 if calculated_power > power else 0\n",
    "\n",
    "    results = (passedPtest, passedPower, passedEffectSize)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vs B Results (P-Test, Power, EffectSize): (1, 1, 1)\n",
      "A vs C Results (P-Test, Power, EffectSize): (0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Compare Group A vs Group B (Should likely be different)\n",
    "results_AB = strengthOfEffect(group_A, group_B, alpha=0.05, power=0.8, effectSize=0.3)\n",
    "print(f\"A vs B Results (P-Test, Power, EffectSize): {results_AB}\")\n",
    "\n",
    "# Test 2: Compare Group A vs Group C (Should likely NOT be different)\n",
    "results_AC = strengthOfEffect(group_A, group_C, alpha=0.05, power=0.8, effectSize=0.5)\n",
    "print(f\"A vs C Results (P-Test, Power, EffectSize): {results_AC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Safe Test - 10pts</h2>\n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values - dataA and dataB.\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll produce a p-value for a two sided hypothesis test:\n",
    "<ul>\n",
    "<li>If the data is not normally distributed, use a Mann-Whitney Test. \n",
    "<li>If the data appears to be normally distributed, and the variance differs substantially, use a Welch's t-test.\n",
    "<li>If none of those conditions are true, use a 'normal' (Student's) t-test. \n",
    "<li>Note: The execution of all of these tests are very similar from your persepective. They are all in the scipy documentation - Google for exact details, and the code closely mirrors the examples we did. \n",
    "<li>Note 2: If you ever need to use a cutoff for a p-value in the middle of your calculations, please choose something reasonable. There are common defaults for whatever you may need. These defaults are likely shown in the documentation or any examples you may look up. \n",
    "</ul>\n",
    "\n",
    "<b>In any case, the value returned is one number (not in a list, tuple, etc...) that is the pValue performed for that test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flexHypTest(dataA, dataB):\n",
    "    # 1. Check Normality (using Shapiro-Wilk)\n",
    "    \n",
    "    statA, p_normA = ss.shapiro(dataA)\n",
    "    statB, p_normB = ss.shapiro(dataB)\n",
    "    \n",
    "    # Cutoff for normality check (standard is 0.05)\n",
    "    normality_cutoff = 0.05\n",
    "    \n",
    "    if p_normA < normality_cutoff or p_normB < normality_cutoff:\n",
    "        # Data is NOT normal -> Use Mann-Whitney U Test\n",
    "        stat, pValue = ss.mannwhitneyu(dataA, dataB, alternative='two-sided')\n",
    "    else:\n",
    "        \n",
    "        statL, p_levene = ss.levene(dataA, dataB)\n",
    "        \n",
    "        if p_levene < 0.05:\n",
    "            # Variances differ substantially -> Welch's t-test (equal_var=False)\n",
    "            stat, pValue = ss.ttest_ind(dataA, dataB, equal_var=False)\n",
    "        else:\n",
    "            # Normal and Equal Variances -> Student's t-test (equal_var=True)\n",
    "            stat, pValue = ss.ttest_ind(dataA, dataB, equal_var=True)\n",
    "    \n",
    "    return pValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-Value (Normal Data): 9.10654008630498e-05\n",
      "P-Value (Non-Normal Data): 2.172254270994724e-23\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Normal Data (A vs B) - Should use T-Test\n",
    "p_val_normal = flexHypTest(group_A, group_B)\n",
    "print(f\"P-Value (Normal Data): {p_val_normal}\")\n",
    "\n",
    "# Test 2: Non-Normal Data (A vs Non-Normal) - Should use Mann-Whitney\n",
    "p_val_non_normal = flexHypTest(group_A, non_normal_data)\n",
    "print(f\"P-Value (Non-Normal Data): {p_val_non_normal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Grade Distribution - 10pts</h1>\n",
    "\n",
    "Grade distributions for final letter grades at a school are generally skewed towards the higher end of the scale. We can model it with a function below.\n",
    "\n",
    "Percentage grades on individual assignments are often skewnormally distributed. (Note: this is more for curved schools than somewhere like NAIT with hard cutoffs. When I was in school CompSci profs would aim for a 50%-60% raw average to get a normal-ish distribution of marks.)\n",
    "\n",
    "You are seeking to generate a grading system, in two steps:\n",
    "<ul>\n",
    "<li>Use the supplied Weibull distribution in the simpleGenerateLetterGradeBuckets function to generate the distribution of letter grades - A,B,C,D,F. We are a simple school and we only have letters, no plus or minus. \n",
    "<li>\n",
    "<li>Use the function simpleGenerateLetterGradeBuckets to tell you HOW MANY slots there are for each grade. This is done for you in the provided function, you just need to call it and get the results. Please pay attention to the n value for number.\n",
    "<li>Take the supplied raw percentage grades and fit them into those buckets. I.E. if there are 17 slots for an A grade, the 17 highest percentage marks should get an A; if there are then 52 for B, then the next 52 highest get a B, etc...\n",
    "<li><b>You are going to return a list of tuples - the original percentage grade, and the letter grade. E.g. [(72,B), (84,A), etc...]</b>\n",
    "</ul>\n",
    "\n",
    "<br><br>\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>A list of raw percentage grades, from 0 to 100. E.g. [100,98,24,53,45, etc...]\n",
    "</ul>\n",
    "\n",
    "You'll produce:\n",
    "<ul>\n",
    "<li>A list of tuples. Each tuple is the original percentage grade, and the letter grade. .\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Note: You'll have to run the function cell down at the bottom first. \n",
    "<br><br>\n",
    "<b>Bonus: The provided function for grade buckets probably isn't the best overall, if you can rewrite it to be better, up to 3 bonus marks. Think about the random factor...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignLetterGrades(rawPercentageGrades):\n",
    "    # 1. Get the bucket counts for the letter grades\n",
    "    n = len(rawPercentageGrades)\n",
    "    buckets = simpleGenerateLetterGradeBuckets(n)\n",
    "    # buckets example: {'A': 10, 'B': 20, ...}\n",
    "    \n",
    "    # 2. Prepare grades for sorting while keeping track of original index\n",
    "  \n",
    "    indexed_grades = []\n",
    "    for i, grade in enumerate(rawPercentageGrades):\n",
    "        indexed_grades.append((grade, i))\n",
    "        \n",
    "    # 3. Sort grades High -> Low\n",
    "    indexed_grades.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # 4. Assign letters based on bucket counts\n",
    "    final_results = []\n",
    "    grade_order = ['A', 'B', 'C', 'D', 'F']\n",
    "    \n",
    "    current_student_idx = 0\n",
    "    \n",
    "    for letter in grade_order:\n",
    "        count = buckets[letter]\n",
    "       \n",
    "        for _ in range(count):\n",
    "            if current_student_idx < len(indexed_grades):\n",
    "                grade_val, original_idx = indexed_grades[current_student_idx]\n",
    "                # Store as (original_index, grade_val, letter)\n",
    "                final_results.append((original_idx, grade_val, letter))\n",
    "                current_student_idx += 1\n",
    "                \n",
    "    # 5. Sort back by original index to maintain the list order \n",
    "   \n",
    "    final_results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Format as requested: [(Percentage, Letter), ...]\n",
    "    listOfTumples = [(x[1], x[2]) for x in final_results]\n",
    "\n",
    "    return listOfTumples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 65, 'B': 173, 'C': 121, 'D': 46, 'F': 18}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example for 423 students\n",
    "simpleGenerateLetterGradeBuckets(423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGenerateLetterGradeBuckets(n=100):\n",
    "    #Define distribution params\n",
    "    c = 1.5\n",
    "    loc = 3\n",
    "    scale = 1.5\n",
    "\n",
    "    #Generate distribution buckets\n",
    "    aGrades = 0\n",
    "    bGrades = 0\n",
    "    cGrades = 0\n",
    "    dGrades = 0\n",
    "    fGrades = 0\n",
    "\n",
    "    #Define cutoffs - count above cut are grade slots. \n",
    "    cuts = [3.7, 2.9, 1.9, .9]\n",
    "    data = 7.2-ss.weibull_min.rvs(c, loc, scale, n)\n",
    "    \n",
    "    #Count the number of slots for each letter grade\n",
    "    for i in range(len(data)):\n",
    "        tmp = data[i]\n",
    "        if tmp > cuts[0]:\n",
    "            aGrades += 1\n",
    "        elif tmp > cuts[1]:\n",
    "            bGrades += 1\n",
    "        elif tmp > cuts[2]:\n",
    "            cGrades += 1\n",
    "        elif tmp > cuts[3]:\n",
    "            dGrades += 1\n",
    "        else:\n",
    "            fGrades += 1\n",
    "    buckets = {\"A\":aGrades, \"B\":bGrades, \"C\":cGrades, \"D\":dGrades, \"F\":fGrades}\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Student Grades (Raw, Letter):\n",
      "(np.int32(81), 'B')\n",
      "(np.int32(48), 'C')\n",
      "(np.int32(89), 'A')\n",
      "(np.int32(66), 'B')\n",
      "(np.int32(97), 'A')\n",
      "(np.int32(41), 'D')\n",
      "(np.int32(44), 'D')\n",
      "(np.int32(68), 'B')\n",
      "(np.int32(76), 'B')\n",
      "(np.int32(77), 'B')\n",
      "\n",
      "Total Counts per Letter:\n",
      "Counter({'B': 85, 'C': 50, 'A': 42, 'D': 17, 'F': 6})\n"
     ]
    }
   ],
   "source": [
    "# Call the function with the random grades we generated\n",
    "final_grades = assignLetterGrades(student_grades)\n",
    "\n",
    "# Print the first 10 results to verify\n",
    "print(\"First 10 Student Grades (Raw, Letter):\")\n",
    "for item in final_grades[:10]:\n",
    "    print(item)\n",
    "\n",
    "# Optional: Print a count of how many of each letter were assigned to verify distribution\n",
    "from collections import Counter\n",
    "letters = [x[1] for x in final_grades]\n",
    "print(\"\\nTotal Counts per Letter:\")\n",
    "print(Counter(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
